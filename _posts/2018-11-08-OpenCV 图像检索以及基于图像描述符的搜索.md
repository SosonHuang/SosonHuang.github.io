---
layout:     post
title:      "OpenCV 图像检索以及基于图像描述符的搜索"
subtitle:   "图像检索以及基于图像描述符的搜索"
date:       2018-11-08
author:     "Soson"
header-img: "img/IMG_9977.JPG"
tags:
    - opencv
---



>特征检测算法

***OpenCV最常用的特征检测和提取算法：***

- Harris：该算法用于检测角点
- SIFT：该算法用于检测斑点
- SURF：该算法用于检测斑点
- FAST：该算法用于检测角点
- BRIEF：该算法用于检测斑点
- ORB：该算法代表带方向的FAST算法与具有旋转不变形的BRIEF算法

***通过以下进行特征匹配：***

>暴力匹配法（Brute-Force）、基于FLANN的匹配法wen
可以采用单应性进行空间验证


多数特征检测算法都会涉及图像的角点、边、和斑点的识别。

***检测焦点特征***

```

import cv2
import numpy as np
import sys

img = cv2.imread('images/chess_board.png')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
gray = np.float32(gray)

#检测焦点特征函数
dst = cv2.cornerHarris(gray, 9, 23, 0.04)
img[dst>0.01 * dst.max()] = [0, 0, 255] 
while (True):
cv2.imshow('corners', img)
if cv2.waitKey(1000 / 12) & 0xff == ord("q"):
break
cv2.destroyAllWindows()
```

* cornerHarris函数最重要是第三个参数，该参数限定了Sobel算子的中孔，Sobel算子通过对图像行，列的变化检测来检测边缘，Sobel算子会通过核来完成检测，cornerHarris函数会使用Sobel算子，并且第三个参数定义了Sobel算子的中孔
* 简单地说，该参数定义了 角点检测的敏感度，其取值必须是介于3和31之间的奇数，如果参数为3，当检测到方块的边界时，棋盘中黑色方块的所有对角线都会被认为是角点。如果参数为23，只有每个方块的角点才会被检测为角点。

![avatar](https://raw.githubusercontent.com/SosonHuang/imagesource/master/11-08/2D5111FE-DD46-4283-8384-AE38A87ED538.png)

>使用DoG和SIFT进行特征提取与描述

- DoG是对同一图像不同高斯滤波器所得到的结果。
- DoG操作的最终结果会得到感兴趣的区域（关键点）。

```
import cv2
import sys
import numpy as np

img = cv2.imread("images/varese.jpg")

gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

sift = cv2.xfeatures2d.SIFT_create()
keypoints, descriptor = sift.detectAndCompute(gray,None)

img = cv2.drawKeypoints(image=img, outImage=img, keypoints = keypoints, flags = 4, color = (51, 163, 236))

cv2.imshow('sift_keypoints', img)
while (True):
if cv2.waitKey(1000 / 12) & 0xff == ord("q"):
break
cv2.destroyAllWindows()
```


如果运行代码出现问题，请参考如下链接找答案：

>https://blog.csdn.net/weixin_43167047/article/details/82841750

![avatar](https://raw.githubusercontent.com/SosonHuang/imagesource/master/11-08/FAFCA26E-6692-483F-A768-8755C80B9A18.png)

SIFT对象会使用DoG检测关键点，并且对每个关键点周围的区域计算特征向量，主要操作：检测和计算。

***关键点剖析***

- pt（点）：属性表示图像中关键点的x坐标和y坐标
- size属性表示特征的直径
- angle属性表示特征的方向，如前面那副被处理过的图像所示
- response属性表示关键点的强度，某些特征会通过SIFT来分类，因为他得到的特征比其他特征更好，通过查看response属性可以评估特征强度。
- octave属性表示特征所在金字塔的层级，SIFT算法与人脸检测算法类似，即只通过改变计算参数来依次处理相同的图像。
- 对象ID（class_id）表示关键点的ID




***使用快速Hessian算法和SURF来提取和检测特征***

SURF采用快熟Hessian算法检测关键点，而SURF提前特征和SIFT很像，SIFT分别采取DoG和SIFT来检测关键点并提取关键点周围的特征。

```
import cv2
import sys
import numpy as np

img = cv2.imread("images/varese.jpg")
alg = "SURF"
hessianValue = 8000

def fd(algorithm):
algorithms = {
"SIFT": cv2.xfeatures2d.SIFT_create(),
"SURF": cv2.xfeatures2d.SURF_create(float(hessianValue) if len(sys.argv) == 4 else 4000),
"ORB": cv2.ORB_create()
}
return algorithms[algorithm]

gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

fd_alg = fd(alg)
keypoints, descriptor = fd_alg.detectAndCompute(gray,None)

img = cv2.drawKeypoints(image=img, outImage=img, keypoints = keypoints, flags = 4, color = (51, 163, 236))

cv2.imshow('keypoints', img)
while (True):
if cv2.waitKey(1000 / 12) & 0xff == ord("q"):
break
cv2.destroyAllWindows()
```
![avatar](https://raw.githubusercontent.com/SosonHuang/imagesource/master/11-08/D9A12F65-504C-4DB8-AE96-CFA95E975152.png)

由SURF算法处理的，所采用的Hessian阈值为8000，阈值越高，能识别的特征就越少，因此可以采取试探法来得到最优的检测。


***基于ORB的特征检测和特征匹配***

- ORB将基于FAST关键点检测的技术和基于BRIEF描述符的技术相结合，有更快的速度。

- FAST算法会在像素周围绘制一个圆，该圆包括16个像素，然后，FAST会将每个像素与加上一个阈值的圆心像素值进行比较，若有连续、比加上一个阈值的圆心的像素值还亮或者暗的像素，则可认为圆心是角点。

- BRIEF并不是特指检测算法，它只是一个描述符，在前面使用SIFT和SURF分析图像时，可以看到整个处理的核心其实是detectAndCompute函数，该函数包括两步：检测和计算，如果将这两步结果以元组形式输出，detectAndCompute函数会返回两种不同的结果。

- 检测结果是一组关键点，计算结果是描述符。这意味着OpenCV的SIFT类和SURF类既是检测器也是描述符。

- 关键点描述符是图像的一种表示，因为可以比较两个图像的关键点描述符，并找到它们的共同之处，所以描述符可以作为特征匹配的一种方法。

>BRIEF是目前最快的描述符。

暴力匹配方法是一种描述符匹配方法，该方法比较两个描述符，并产生匹配的结果的列表。称为暴力匹配的原因是该算法基本上不涉及优化，第一个描述符的所有特征都用来和第二个描述符的特征进行比较。每次比较都会给出一个距离值，而最好的比较结果会被认为是一个匹配。



***ORB特征匹配***


```
import numpy as np
import cv2
from matplotlib import pyplot as plt

# query and test images
img1 = cv2.imread('images/manowar_logo.png',0)
img2 = cv2.imread('images/manowar_single.jpg',0)

# create the ORB detector
orb = cv2.ORB_create()
#开始创建ORB特征检测器和描述符
#与SIFT和SURF的操作一样，对查询图像和训练图像都进行检查，然后计算关键点和描述符
kp1, des1 = orb.detectAndCompute(img1,None)
kp2, des2 = orb.detectAndCompute(img2,None)

# brute force matching  暴力匹配
bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
matches = bf.match(des1,des2)

# Sort by distance.
matches = sorted(matches, key = lambda x:x.distance)
img3 = cv2.drawMatches(img1,kp1,img2,kp2, matches[:25], img2,flags=2)

plt.imshow(img3),plt.show()
```

![avatar](https://raw.githubusercontent.com/SosonHuang/imagesource/master/11-08/96C60913-4F96-4589-94E3-02BA370EAAF1.png)


***K-最近邻匹配（匹配检测算法）***


```
import numpy as np
import cv2
from matplotlib import pyplot as plt

# query and test images
img1 = cv2.imread('images/manowar_logo.png',0)
img2 = cv2.imread('images/manowar_single.jpg',0)

# create the ORB detector
orb = cv2.ORB_create()
kp1, des1 = orb.detectAndCompute(img1,None)
kp2, des2 = orb.detectAndCompute(img2,None)

# brute force matching
bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
matches = bf.knnMatch(des1,des2, k=1)

img3 = cv2.drawMatchesKnn(img1,kp1,img2,kp2, matches, img2,flags=2)

plt.imshow(img3),plt.show()

```
![avatar](https://raw.githubusercontent.com/SosonHuang/imagesource/master/11-08/96C60913-4F96-4589-94E3-02BA370EAAF1.png)

match函数和knnMatch函数的区别在于：match函数返回最佳匹配，KNN函数返回k个匹配，开发人员用knnMatch可进一步处理这些匹配，例如可以遍历匹配，并采取比率测试来过滤掉不满足用户定义条件的匹配


***FLANN匹配***

>FLANN匹配具有一种内部机制，该机制可以根据数据本身选择最适合的算法来处理数据集。经验证，FLANN比其他的最近邻搜索软件快10倍。

GitHub地址：https://github.com/mariusmuja/flann

```
import numpy as np
import cv2
from matplotlib import pyplot as plt

queryImage = cv2.imread('images/bathory_album.jpg',0)
trainingImage = cv2.imread('images/bathory_vinyls.jpg',0)

# create SIFT and detect/compute
sift = cv2.xfeatures2d.SIFT_create()
kp1, des1 = sift.detectAndCompute(queryImage,None)
kp2, des2 = sift.detectAndCompute(trainingImage,None)

# FLANN matcher parameters
# FLANN_INDEX_KDTREE = 0
indexParams = dict(algorithm = 0, trees = 5)
searchParams = dict(checks=50)   # or pass empty dictionary

flann = cv2.FlannBasedMatcher(indexParams,searchParams)

matches = flann.knnMatch(des1,des2,k=2)

# prepare an empty mask to draw good matches
matchesMask = [[0,0] for i in xrange(len(matches))]

# David G. Lowe's ratio test, populate the mask
for i,(m,n) in enumerate(matches):
if m.distance < 0.7*n.distance:
matchesMask[i]=[1,0]

drawParams = dict(matchColor = (0,255,0),
singlePointColor = (255,0,0),
matchesMask = matchesMask,
flags = 0)

resultImage = cv2.drawMatchesKnn(queryImage,kp1,trainingImage,kp2,matches,None,**drawParams)

plt.imshow(resultImage,),plt.show()
```

![avatar](https://raw.githubusercontent.com/SosonHuang/imagesource/master/11-08/3B237A64-715E-4908-B09C-A72A06D69ABF.png)
FLANN匹配器有两个参数：indexParrams和searchParams


***FLANN的单应性匹配***

>单应性是一个条件，该条件表明当两幅图像中的一副出现投影畸变时，它们还能彼此匹配。

```
import numpy as np
import cv2
from matplotlib import pyplot as plt

MIN_MATCH_COUNT = 10

img1 = cv2.imread('images/bb.jpg',0)
img2 = cv2.imread('images/color2_small.jpg',0)

# Initiate SIFT detector
sift = cv2.xfeatures2d.SIFT_create()

# find the keypoints and descriptors with SIFT
kp1, des1 = sift.detectAndCompute(img1,None)
kp2, des2 = sift.detectAndCompute(img2,None)

FLANN_INDEX_KDTREE = 0
index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
search_params = dict(checks = 50)

flann = cv2.FlannBasedMatcher(index_params, search_params)

matches = flann.knnMatch(des1,des2,k=2)

# store all the good matches as per Lowe's ratio test.
good = []
for m,n in matches:
if m.distance < 0.7*n.distance:
good.append(m)

if len(good)>MIN_MATCH_COUNT:
src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)
dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)

M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)
matchesMask = mask.ravel().tolist()

#单应性
h,w = img1.shape
pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)
dst = cv2.perspectiveTransform(pts,M)

#投影畸变
img2 = cv2.polylines(img2,[np.int32(dst)],True,255,3, cv2.LINE_AA)

else:
print "Not enough matches are found - %d/%d" % (len(good),MIN_MATCH_COUNT)
matchesMask = None

draw_params = dict(matchColor = (0,255,0), # draw matches in green color
singlePointColor = None,
matchesMask = matchesMask, # draw only inliers
flags = 2)

img3 = cv2.drawMatches(img1,kp1,img2,kp2,good,None,**draw_params)

plt.imshow(img3, 'gray'),plt.show()

```

![avatar](https://raw.githubusercontent.com/SosonHuang/imagesource/master/11-08/545B0B41-86F0-43CB-BC54-C5BA944F38CA.png)
与前面基于FLANN匹配的例子相比，唯一的差别（这是产生所有操作的地方）在if模块。

>if len(good)>MIN_MATCH_COUNT:

要确保至少有一定数目的良好匹配（计算单应性最少需要4个匹配），将其设定为10.

***在原始图像和训练图像中发现关键点：***

```
src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)
dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)
```

***基于纹身取证的示例***

假如你就职于广州市的取证部门，需要识别纹身，已经有罪犯纹身的原始照片，虽然还不知道罪犯的身份，但已经拥有了纹身数据库，因此可以通过纹身照片来查询纹身者的名字。

- 任务分两部分：将图像描述符保存到文件中，然后用该照片作为查询图像，在数据库中检索匹配的图像。

>创建描述符的过程：加载图像、创建特征、检测器、检测并计算。


```
import os
import cv2
import numpy as np
from os import walk
from os.path import join
import sys

def create_descriptors(folder):
files = []
for (dirpath, dirnames, filenames) in walk(folder):
files.extend(filenames)
for f in files:
save_descriptor(folder, f, cv2.xfeatures2d.SIFT_create())

def save_descriptor(folder, image_path, feature_detector):
print "reading %s" % image_path
if image_path.endswith("npy"):
return
img = cv2.imread(join(folder, image_path), 0)
keypoints, descriptors = feature_detector.detectAndCompute(img, None)
descriptor_file = image_path.replace("jpg", "npy")
np.save(join(folder, descriptor_file), descriptors)

dir = sys.argv[1]


if __name__ == "__main__":
create_descriptors(dir)
```

需要将存放所有图像的文件夹名传递给这个脚本，然后在同一个文件夹中创建所有的描述符文件。
Numpy有一个save()函数，可以采用优化方式将数组数据保存到一个文件中。

- 运行如下命令：

>python generate_descriptors.py 目录名称

***扫描匹配***

>描述符文件保存到文件后，对所有描述符进行单应性处理。
步骤：

- 加载查询图片，并为其创建描述符
- 扫描文件夹中的描述符
- 对于每一个描述符，通过FLANN来计算匹配
- 如果匹配数目超过了给定阈值，包括潜在罪犯档案
- 对于所有匹配的罪犯，认为匹配数量最多的图像是潜在的犯罪嫌疑人

```
from os.path import join
from os import walk
import numpy as np
import cv2
from sys import argv

# create an array of filenames
folder = "anchors"
query = cv2.imread(join(folder, "tattoo_seed.jpg"), 0)

# create files, images, descriptors globals
files = []
images = []
descriptors = []
for (dirpath, dirnames, filenames) in walk(folder):
files.extend(filenames)
for f in files:
if f.endswith("npy") and f != "tattoo_seed.npy":
descriptors.append(f)
print descriptors

# create the sift detector
sift = cv2.xfeatures2d.SIFT_create()
query_kp, query_ds = sift.detectAndCompute(query, None)

# create FLANN matcher
FLANN_INDEX_KDTREE = 0
index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
search_params = dict(checks = 50)
flann = cv2.FlannBasedMatcher(index_params, search_params)

# minimum number of matches
MIN_MATCH_COUNT = 10

potential_culprits = {}

print ">> Initiating picture scan..."
for d in descriptors:
print "--------- analyzing %s for matches ------------" % d
matches = flann.knnMatch(query_ds, np.load(join(folder, d)), k =2)
good = []
for m,n in matches:
if m.distance < 0.7*n.distance:
good.append(m)
if len(good) > MIN_MATCH_COUNT:

print "%s is a match! (%d)" % (d, len(good))
else:
print "%s is not a match" % d
potential_culprits[d] = len(good)

max_matches = None
potential_suspect = None
for culprit, matches in potential_culprits.iteritems():
if max_matches == None or matches > max_matches:
max_matches = matches
potential_suspect = culprit

print "potential suspect is %s" % potential_suspect.replace("npy", "").upper()
```



![avatar](https://raw.githubusercontent.com/SosonHuang/imagesource/master/11-08/D366FFB1-A6BB-42AF-BBE5-3A84CE8F8003.png)



